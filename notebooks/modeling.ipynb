{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letterboxd Analysis Project: Modeling\n",
    "\n",
    "**Author:** Sierra Stanton\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Theater Scene](../images/zach-galifianakis-math.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll initially determine a simple recommendation model and iteratively build on our efforts to improve the film recommendations for Letterboxd users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtual Environment\n",
    "\n",
    "In order to ensure you have the required packages to run the code in this notebook, an environment.yaml file is __[here](https://github.com/sierrafromcalifornia/Letterboxd-Recommender-Project#environment)__ for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard import\n",
    "import pandas as pd\n",
    "\n",
    "# import needed surprise libraries\n",
    "from surprise import Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.prediction_algorithms import SVD, KNNWithMeans, KNNBasic\n",
    "\n",
    "# retrieve dataframe as pickle file\n",
    "import pickle\n",
    "df = pickle.load(open(\"df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ensure our resulting dataset is brought in for modeling\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've verified that the data we feed into our read has the following required Surprise columns present: `user ; item ; rating ;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our modeling, we choose to use the __[Surprise library](https://surprise.readthedocs.io/en/stable/index.html)__, a Python sci-kit for recommender systems. The library contains built in algorithms and cross-validation methods we can use to make an increasingly proficient model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in values as surprise dataset\n",
    "reader = Reader(rating_scale=(1,10), line_format=('item rating user'))\n",
    "data = Dataset.load_from_df(df[['user_id', 'movie_id', 'rating_val']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "train, test = train_test_split(data, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.trainset.Trainset at 0x7ff6c56fcc50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our first model: KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Surprise library, `KNNBasic` will give us a basic collaborative filtering algorithm to start with. Let's see how our data starts performing with these algorithms so we can continue fine tuning and get the best recommendations possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Nieghbor methods do X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little on similarity.\n",
    "\n",
    "\n",
    "Similarities:\n",
    "* **cosine**:\tCompute the cosine similarity between all pairs of users (or items).\n",
    "* **pearson**:\tCompute the Pearson correlation coefficient between all pairs of users (or items).\n",
    "* **msd**:\tCompute the Mean Squared Difference similarity between all pairs of users (or items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sim_options = {'name': 'cosine'}\n",
    "algo_knn_basic = KNNBasic(sim_options=sim_options)\n",
    "predictions = algo_knn_basic.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE = 1.5951053223214062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we change the similarity matrix. While cosine does [explainer], the pearson similarity measure [explainer]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sim_options = {'name': 'pearson'}\n",
    "algo_knn_basic = KNNBasic(sim_options=sim_options)\n",
    "predictions = algo_knn_basic.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE 1.5884529441409094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sim_options = {'name': 'msd'}\n",
    "algo_knn_basic = KNNBasic(sim_options=sim_options)\n",
    "predictions = algo_knn_basic.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.488729665653617"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE 1.4898636221946142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Baseline Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### So, what exactly does our RMSE tell us?\n",
    "\n",
    "Our RMSE shows us the typical amount our model prediction differs from the actual rating a user would give by comparing our predictors with our test data for accuracy.\n",
    "\n",
    "Since our best performing model so far has a 1.4899 RMSE, our predicted user rating for a film is already typically less than a star from the reality. Let's use what we've learned to see how we can model by trying different algorithms and parameters.\n",
    "\n",
    "Across our trial and error models, we actually tried all the similarity metrics available in the surprise library but found the two above to give us the best results so far. They can be found __[here](https://surprise.readthedocs.io/en/stable/similarities.html)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What do our predictions look like in real time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Prediction(uid='rbonaime', iid='the-bigger-picture', r_ui=6.0, est=6.853587940579535, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid='allisoncm', iid='the-technique-and-the-rite', r_ui=5.0, est=6.0, details={'actual_k': 1, 'was_impossible': False}),\n",
       " Prediction(uid='joedicanio', iid='may', r_ui=7.0, est=6.605265251368509, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid='dylanblondee', iid='22-july', r_ui=5.0, est=6.33035490908406, details={'actual_k': 40, 'was_impossible': False}),\n",
       " Prediction(uid='tubbs', iid='the-sisterhood-of-the-traveling-pants', r_ui=4.0, est=5.3535220692543914, details={'actual_k': 40, 'was_impossible': False})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output example predictions within our current model\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's predict how a particular user might rate a particular film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'davidehrlich'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for this user\n",
    "df['user_id'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiest-season'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for this item\n",
    "df['movie_id'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='davidehrlich', iid='happiest-season', r_ui=None, est=6.06895844485103, details={'actual_k': 40, 'was_impossible': False})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's our predicted rating\n",
    "algo_knn_basic.predict(df['user_id'][1], df['movie_id'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating our first model, we currently predict user `davidehrlich` would rate Hulu film Happiest Season with a 6/10 aka 3 stars in the Letterboxd app. We can use these predictions to order films to serve him the recommendations that will better fit his taste. Yet, we need to keep in mind that improved accuracy will make this feature more powerful and poignant. Narratives have a special place in our society and with a database of over 250K films to recommend - let's try to further increase our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our second model: KNNWithMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KNNWithMeans` is also a basic collaborative filtering algorithm, but this algorithm takes into account the mean ratings of each user. We'll see if this algorithm brings better results across the two similarity metrics that have proved most successful to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k = 15\n",
    "min_k = 5\n",
    "sim_options = {'name': 'pearson'}\n",
    "knn_means = KNNWithMeans(k=k, min_k=min_k, sim_options=sim_options, verbose=True)\n",
    "    \n",
    "predictions = knn_means.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE: 1.5607923504104053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k = 15\n",
    "min_k = 5\n",
    "sim_options = {'name': 'msd'}\n",
    "knn_means = KNNWithMeans(k=k, min_k=min_k, sim_options=sim_options, verbose=True)\n",
    "    \n",
    "predictions = knn_means.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE 1.514157952922513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "k = 40\n",
    "min_k = 5\n",
    "sim_options = {'name': 'msd'}\n",
    "knn_means = KNNWithMeans(k=k, min_k=min_k, sim_options=sim_options, verbose=True)\n",
    "    \n",
    "predictions = knn_means.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE 1.508629823133224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if using the similarity type that's winning so far aids the first model we tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A similarity iteration with our first model: KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sim_options = {'name': 'msd'}\n",
    "algo_knn_basic = KNNBasic(sim_options=sim_options)\n",
    "predictions = algo_knn_basic.fit(train).test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE 1.4898636221946142"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we brought the winning similarity metric from our KNNWithMeans iterations back to our first model, we clearly improved our RMSE - which previously had a record of 1.5884 (with KNNBasic) and 1.5086 (with KNNWithMeans)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our third model: SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SVD`, which was originally popularized by Simon Funk during the Netflix Prize competition (__[see Simon's breakdown of this model](https://sifter.org/simon/journal/20061211.html)__), is now used across a variety of applications.\n",
    "\n",
    "Unlike the prior two models we tried above - this algorithm is X based instead of focused around similarity metrics.\n",
    "\n",
    "A little about bein gMatrix Factorization-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "svd = SVD()\n",
    "svd.fit(train)\n",
    "predictions = svd.test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE: 1.4183454579066075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best RMSE to date! Our model prediction results would place us within about .70 stars off from the reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing our Default Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Surprise library has methods to help us find the best parameters for tuning our model. I ran surprise's `GridsearchCV` method across multiple quantities and factors for the SVD algorithm in the notebook titled [Gridsearch](/notebooks/gridsearch.ipynb) and extracted the best performing parameters. Let's run these new parameters on our model below and see how our accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# fit our algorithm with winning hyperparameters\n",
    "svd = SVD(lr_all=0.008, n_factors=40, reg_all=0.025, biased=True)\n",
    "svd.fit(train)\n",
    "predictions = svd.test(test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4021701605968926"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* RMSE: 1.4021701605968926"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully used Gridsearch to identify better parameters for our model! This is the highest RMSE we've achieved so far and the run time was under twenty minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's predict how that same user might rate the same film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'davidehrlich'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for this user\n",
    "df['user_id'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiest-season'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for this item\n",
    "df['movie_id'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='davidehrlich', iid='happiest-season', r_ui=None, est=5.488776723015272, details={'was_impossible': False})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's our predicted rating\n",
    "svd.predict(df['user_id'][1], df['movie_id'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see with greater accuracy that user `davidehrlich` is estimated to rate Hulu film Happiest Season with a 5.5 on a 10 point scale aka 2.5 stars in Letterboxd. With our first model, we predicted 2.5 stars and have gotten increasingly closer to what our user likely would've rated the film. Now, we can surface better recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
