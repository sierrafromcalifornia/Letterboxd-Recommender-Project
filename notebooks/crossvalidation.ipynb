{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation: Finding our Best Hyperparameters for SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our best performing model to date, `SVD`, and aim to further improve upon the default hyperparameters included in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import needed surprise libraries\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "# retrieve pickle file\n",
    "import pickle\n",
    "df = pickle.load(open(\"df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating_val</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>8</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>7</td>\n",
       "      <td>davidehrlich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>4</td>\n",
       "      <td>ingridgoeswest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>7</td>\n",
       "      <td>silentdawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>2</td>\n",
       "      <td>colonelmortimer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie_id  rating_val          user_id\n",
       "0  happiest-season           8       deathproof\n",
       "1  happiest-season           7     davidehrlich\n",
       "2  happiest-season           4   ingridgoeswest\n",
       "3  happiest-season           7       silentdawn\n",
       "4  happiest-season           2  colonelmortimer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of our dataframe, 8M represented, take 10% for gridsearch\n",
    "#df_sub = df.copy()\n",
    "#df_sub = df.sample(5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the code above allows us to take a subset of data for our gridsearch, which is often helpful with larger datasets. However, with our dataset it was noticeably a higher RMSE with our subsets and I decided to run several gridsearch's across our full set within the time I had to do this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in values as surprise dataset\n",
    "reader = Reader(rating_scale=(1,10), line_format=('item rating user'))\n",
    "data = Dataset.load_from_df(df[['user_id', 'movie_id', 'rating_val']],reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Weekend Search](../images/theweekend_superbowl.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the Surprise library, The `GridSearchCV` class computes accuracy metrics for an algorithm on various combinations of parameters, over a cross-validation procedure. This proves useful for finding the best set of parameters for a prediction algorithm.\n",
    "\n",
    "This process can be especially time-consuming when dealing with over 8M rows of film ratings data. I developed a process through trial and error that can help others bring structure to their search and perhaps newfound insight into the data you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the key default parameters of an SVD model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Default SVD](../images/default-svd-model-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default SVD model in our `modeling` notebook, we achieved a RMSE of 1.4183 (on our elongated ten-point scale) so we can typically predict a user's rating on a five-star scale with an accuracy of less than one star away.\n",
    "\n",
    "With film recommendation systems, it's still especially helpful to understand your data to a particular granularity in order to better gauage what kind of films will be served to the end-user. Noting factors like behavioral psycology (i.e. recency bias) and qualities to the film's included in your database is helpful. Often, a service like Netflix, would serve recommendations with associated filters that can provide helpful clusters rather than a smorgasboard of films or simply the most popular (where we have the most associated ratings). The __[cold start problem](https://www.kdnuggets.com/2019/01/data-scientist-dilemma-cold-start-machine-learning.html)__ is worth noting and we're especially learning how many ratings we might need on the front-end to get an accurate enough picture of a user in order to provide increasingly helpful recommendations.\n",
    "\n",
    "Now, let's see what our quantitative measures tell us below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this method is considered exhaustive ‚è∞ and can take hours to run, I'll comment out the code below with the accompanying results in order to showcase development and aid anyone who'd like to follow suit.\n",
    "\n",
    "__[Here](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv)__ is the accompanying documentation where an example of this process is shown from surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# print our best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# print the combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.5696\n",
    "* Best Parameters: 'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4\n",
    "\n",
    "So far, we can see all the winning parameters are trending toward the default SVD parameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [25, 27], 'lr_all': [0.0093, 0.0095],\n",
    "              'reg_all': [0.01, 0.02]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])                                \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4772\n",
    "* Best Parameters: 'n_epochs': 25, 'lr_all': 0.0093, 'reg_all': 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [25, 26], 'lr_all': [0.0093, 0.009],\n",
    "              'reg_all': [0.015, 0.02]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse']) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4760\n",
    "* Best Parameters: 'n_epochs': 25, 'lr_all': 0.009, 'reg_all': 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Gridsearches above show us that many of our default parameters are indeed preferred, with the exception of our learning rate which could actually be improved upon with just a slight decrease.\n",
    "\n",
    "Because this is still not an improvement on our initial model - let's experiment with a parameter we haven't tuned quite yet - `n_factors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'lr_all': [0.009], 'n_factors':[50,100,150]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse']) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4414\n",
    "* Best Parameters: 'lr_all': 0.009, 'n_factors': 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
