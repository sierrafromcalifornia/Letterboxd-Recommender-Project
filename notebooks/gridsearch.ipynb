{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation via Gridsearch: Finding our Best Parameters for SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use our best performing model to date, `SVD`, and aim to further improve upon the default hyperparameters included in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import needed surprise libraries\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "# retrieve pickle file\n",
    "import pickle\n",
    "df = pickle.load(open(\"df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating_val</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>8</td>\n",
       "      <td>deathproof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>7</td>\n",
       "      <td>davidehrlich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>4</td>\n",
       "      <td>ingridgoeswest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>7</td>\n",
       "      <td>silentdawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happiest-season</td>\n",
       "      <td>2</td>\n",
       "      <td>colonelmortimer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie_id  rating_val          user_id\n",
       "0  happiest-season           8       deathproof\n",
       "1  happiest-season           7     davidehrlich\n",
       "2  happiest-season           4   ingridgoeswest\n",
       "3  happiest-season           7       silentdawn\n",
       "4  happiest-season           2  colonelmortimer"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a subset of our dataframe, 8M represented, take 10% for gridsearch\n",
    "#df_sub = df.copy()\n",
    "#df_sub = df.sample(5000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the code above allows us to take a subset of data for our gridsearch, which is often helpful with larger datasets. This is separate from the fold you can declare with `cv` below. In my case, as often occurs, there was a noticeably higher RMSE when I used a subset of data so I decided to use `cv=3` instead across our full dataframe. The run time for these cells are lengthy (none are under an hour) so if you'd like to run them - just uncomment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in values as surprise dataset\n",
    "reader = Reader(rating_scale=(1,10), line_format=('item rating user'))\n",
    "data = Dataset.load_from_df(df[['user_id', 'movie_id', 'rating_val']],reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The Weekend Search](../images/theweekend_superbowl.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the Surprise library, The `GridSearchCV` class computes accuracy metrics for an algorithm on various combinations of parameters, over a cross-validation procedure. This proves useful for finding the best set of parameters for a prediction algorithm.\n",
    "\n",
    "This process can be especially time-consuming when dealing with over 8M rows of film ratings data. By the end of this notebook, through trial and error, I developed a process that can help others bring structure to their search and perhaps newfound insight into the data you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at some of the key default parameters of an SVD model that we'll be altering in this notebook:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Default SVD](../images/default-svd-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the ratings data we imported is on an elongated ten point scale (vs the five stars in Letterboxd) in order to account for the .5 increments that can be rated between stars.\n",
    "\n",
    "With the default SVD model in our `modeling` notebook, we achieved a RMSE of 1.4183 (on our elongated ten-point scale) so we can typically predict a user's rating on a five-star scale with an accuracy of less than one star away.\n",
    "\n",
    "With film recommendation systems, it's still especially helpful to understand your data at a level of granularity in order to better gauage what kind of films will be served to the end-user. Noting factors like behavioral psycology (i.e. recency bias) and attributed film qualities is helpful. Often, a service like Netflix, attempts to serve recommendations with associated filters that can provide helpful clusters rather than a smorgasboard of films or solely the most popular (where we often have the most associated ratings). The __[cold start problem](https://www.kdnuggets.com/2019/01/data-scientist-dilemma-cold-start-machine-learning.html)__ is worth noting and we're especially learning how many ratings we might need on the front-end from a user to get an accurate enough picture so we can provide increasingly helpful recommendations. There's a bit of a balancing act here.\n",
    "\n",
    "Now, let's see what our quantitative measures tell us below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⏰ NOTE: Because this method is considered exhaustive ⏰ and can take hours to run, I'll comment out the code below with the accompanying results in order to showcase development and aid anyone who'd like to follow suit.\n",
    "\n",
    "__[This accompanying documentation](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv)__ from Surprise sheds light on the GridSearch process with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# print our best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# print the combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.5696\n",
    "* Best Parameters: 'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4\n",
    "\n",
    "From our first Gridsearch, we tested parameters and were able to plug in some of the default SVD parameters as well as other stats. So far, we can see all the winning parameters are trending toward the default SVD parameters in our graphic above.\n",
    "\n",
    "Let's try some more comparisons below to see which paramaters most improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [25, 27], 'lr_all': [0.0093, 0.0095],\n",
    "              'reg_all': [0.01, 0.02]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])                                \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4772\n",
    "* Best Parameters: 'n_epochs': 25, 'lr_all': 0.0093, 'reg_all': 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'n_epochs': [25, 26], 'lr_all': [0.0093, 0.009],\n",
    "              'reg_all': [0.015, 0.02]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse']) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4760\n",
    "* Best Parameters: 'n_epochs': 25, 'lr_all': 0.009, 'reg_all': 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Gridsearches above show us that many of our default parameters are indeed preferred, with the exception of our learning rate which could actually be improved upon with just a slight decrease.\n",
    "\n",
    "Because this could likely still be closer to the initial RMSE in SVD model - let's experiment with a parameter we haven't tuned quite yet - `n_factors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'lr_all': [0.009], 'n_factors':[50,100,150]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse']) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4414\n",
    "* Best Parameters: 'lr_all': 0.009, 'n_factors': 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lower `n_factors` gives us a better RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Further Improve our Approach\n",
    "At this point I revisited our potential SVD parameters in Surprise, identified a greater range of notable factors and brought a range of 2-3 values below to test across. This cell below took over 30 hours to run but clearly brought our best RMSE so far and is now worth running across our full training set in our [modeling notebook](/notebooks/modeling.ipynb).\n",
    "\n",
    "You can see that our learnings above were brought in and tested with a range of values surrounding those winning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4214825737387378\n",
      "{'lr_all': 0.008, 'n_factors': 40, 'reg_all': 0.025, 'biased': True}\n",
      "<surprise.prediction_algorithms.matrix_factorization.SVD object at 0x7fe59ac31438>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# tune hyperparameters using GridSearch to get improved model\n",
    "\n",
    "param_grid = {'lr_all': [0.008,0.009,0.0092], 'n_factors': [40,50,75],\n",
    "             'reg_all': [0.0175,0.02,0.025], 'biased': [True, False]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "# best accuracy results for the chosen measure, averaged over all splits\n",
    "print(gs.best_estimator['rmse'])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "* Best RMSE: 1.4215\n",
    "* Best Parameters: 'lr_all': 0.008, 'n_factors': 40, 'reg_all': 0.025, 'biased': True\n",
    "\n",
    "**Our best RMSE from Gridsearch to date!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this tell us?\n",
    "From our experimented parameters grids above, we ended up determining that our best parameters are the following: \n",
    "\n",
    "##### 'lr_all': 0.008, 'n_factors': 40, 'reg_all': 0.025, 'biased': True\n",
    "\n",
    "It's important to note that since our grid was only tested across a third of the full dataset (reference: `cv=3`) - we'll want to bring these parameters across our full train set in our modeling notebook to test how RMSE might change when fed a larger set of data.\n",
    "\n",
    "With more time, we could go past these parameters into more granular hypertuning and see how else we can further our model's accuracy. See the __[full list of parameters](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)__ we can potentially tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
